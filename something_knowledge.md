1.为什么需要对数值类型的特征做归一化？
  因为大的数值在模型中起到的作用可能会更大，小的起得作用会小；比如逻辑回归。

2.怎样处理类别型特征？
因为类别型特征是离散的，我们将其编码，可以独热表示，也可以表示为二进制，目前的稀疏矩阵，在sklearn之中有自己的计算方法，所以我更愿意选择其为我的表示方法。

3.什么是组合特征？如何处理高维组合特征？
  组合特征是两个属性联合起来，而在NLP中可以是词加词性，对于高维的组合特征，把其表示为对应规模的低维向量。

4.怎样有效的找到组合特征？
  用决策树进行特征筛选，筛选出相应路径。

5.有哪些文本表示模型？他们各有什么优缺点？
  比如最简单的词袋。
  再比如再简单的one-hot.
  然后是n-gram模型。
  最后是传说中的wordembedding。
  其中典型代表是skip-gram和CBOW

（1).一个是用周围的词去预测中间的词，投影层是前面层的和，输出层是一个softmax,预测其是词表中的词的概率。它的梯度会先更新加和的sum的值，然后再更新周围的词。
  (2).skip是用当前词去预测周围的词，它的投影层就是普通的投影层，但输出的概率是在当前词下周围词的概率。
  (3).但是如果用softmax的话，我们会发现计算量太过庞大了。所以人们提出用霍夫曼树来优化这个过程。霍夫曼树是按照词在语料中的词频，来决定词在霍夫曼树中的位置。霍夫曼树的父亲节点,是两个节点霍夫曼树节点的和。
  (4).在计算了霍夫曼树之后，我们预测的是向左还是向右，如果是1，就想做走。那问题就转为了向左向右的概率。
  (5).那么softmax词输出的是sum的值，而之后每一个节点都对应一个logistic，逻辑有自己的参数，而词向量是一个logistic的输入。
 （6).然后再考虑logistic的P，分别代表向左向右，那么我们可以用最大似然法来计算。
 （7).首先是最大似然是乘积计算的，所有的概率不断相乘，希望能够最大化概率，通过更改参数的方式。
  (8).但为了便于计算，我们加上log,把其变成连加，然后再进行反向传播，反向传播更新log的参数，
  (9).首先针对每一层进行更新，计算值，最大似然log,然后积累e和参数并进行更新
  (10).然后每一步积累的e,最后对每一个词向量加上去，进行更新。
  (11).对一个样例操作完了，对所有的样例都进行该操作，最后不断的对整个语料库进行操作，最后直到收敛。
  (12).那么skip-gram模型的层次softmax该怎么做呢？理论上我们预测的是在当前词的情况下的概率，但为了进行更新，我们预测的是在周围词的情况下当前词的概率，这样就能达到一个更新周为此的效果。虽然P变了，但最终我们还是同样的方法进行更新。
  (13).我们期望正例满足：正例和参数的逻辑回归是1，而负例是0，那么就可以写出似然函数，可以写出对数似然函数，可以推导出公式。
  
  （14).反向的时候就不断积累e,积累参数，然后更新每一个词向量。
   （15).skip的时候，是针对于每个词，我针对于每个neg,进行诸如此类的操作。同样，上述的skip也要进行该操作。
  （16).那么如何采样呢，很简单，将词按照不同的磁场划分为不同的分数，然后用一个远大于N的数字M，进行划分，然后按照比例提取出数字，最后进行输出。
   （17).
6.如何缓解图像分类任务中训练数据不足带来的问题？
    transform和锐化，钝化，旋转，甚至是使用生成模型
7.Word2Vec是如何工作的？它和隐狄利克雷模型有什么区别和联系？
    他的工作原理见上者，和LDA没有什么不同。
2.模型评估
1.准确率的局限性？
    在准确率有时候很高，但是它的实际意义并不是很高，有时候准确率很高，但我们的局限性却很小。
2.精确率和召回率的平衡？
    精确率是指判断为正的样本中真正为正的是多少
    召回率是指判断为负的样本中真正为负的是多少
3.平方根误差的意外？
   在我们使用平方根误差进行线性回归的时候，我们用的这个方程，但是如果存在较大的噪声点的时候，我们就会发现我们的判别很有问题。那么该怎么做呢？其一，是修改噪声点，其二是加入噪声点的特征处理，其三，是加入噪声点的处理。
4.什么是ROC曲线？
   ROC曲线是一个神秘的曲线，该曲线能达到的效果是在P-R曲线不同的效果，他用真正例和假正例，作为一个衡量标准，最后不断的提高判断的阈值，达成一个效果。这个效果比P-R曲线更加稳定，因为P-R曲线针对不同比例的测试集会表现出不同的形态，而针对ROC，我们会发现，它会表现的更好些。为什么用F1-score,其实也是为了综合考量两个指标。
5.为什么要进行A/B测试？
  其实就是一个控制变量法，我们会发现在处理的过程中，测试两个算法，最好把他们投入到实际的应用中去。
6.过拟合和欠拟合具体指什么现象？
  过拟合是指模型太过复杂，而欠你合是指模型太过普通。
  所以从三个角度考虑：输入，模型复杂度，损失函数。
  如果过拟合，我们可以考虑减少一些特征，或者增加一些噪音，增多数据集。
  如果模型复杂度过高，我们可以减少模型参数，决策树剪枝，神经网络减层。
  如果过拟合，我们可以增加L1正则化，保证情况的稳定。
  
7.如何绘制ROC曲线？
  首先X是错正阳李，Y是正确的正样例的比例，根据阈值而变卦，那么很简单，连上这些线就可以了。
8.如何计算AUC？
  AUC其实是ROC地下的面积，只要用微分公式进行面积的计算就可以了。
9.为什么在一些场景中要使用余弦相似度而不是欧式距离？
  因为余弦相似度它是带方向的，比如有的情况下值很接近，但预先相似度不同。
10.如何划分实验组和对照组？
  那就一个字，控制变量。
11.模型评估中的验证方法及其优缺点？
  1.houdout算法
  2.cv算法
  3.自助法，有放回的抽样
12.能否说出几种降低过拟合和欠你合风险的方法?
   省略
13.ROC曲线和P-R曲线有什么特点？
14.余弦距离是否是一个严格定义的问题？
   不是，他不满足三角形不等式，比如相等，对称，三角形不等死
15.自主法采样在基线情况下会有多少数据从未被选择过？
   36%左右
16.超参数有哪些调参方法？
   1.网格搜索
   2.随机搜索
   3.朴素贝叶斯搜索
17.


多层感知机表示异或逻辑结构，需要几个隐含层？
    至少需要一个隐藏层。
    首先，从数学层面上理解，对于线型分类器，在正向上是单调的，尤其对于异或问题，那么只有一个是影响的，加上sigmoid也是递增的，所以随着x的增长，必然增长，那么
当然无法解决这个分类问题。
19.


20.clip_gradient
首先设置一个阈值a
计算l2参数，当其超过这个阈值的时候
用 阈值/l2 再乘以梯度g来得到结果。

21.写出常用的激活函数和导数：
  1.f(z) = 1/(1+exp(-z)  则导数为f(z)(1-f(z))
  2.f(z) = tanh(z) = e(z) - e(-z) / e(z) + e(-z) 其导数为 f'(z) = 1 - f(z)的二次方
  3.f(z) = max(0,z)
    f(z) = 1 or 0
    
    
22.为什么Sigmoid和Tanh激活函数会导致梯度消失的现象？
   首先在sigmoid中，我们会发现sigmoid中的值在趋向于无穷的时候，它的值是1，这个时候它几乎是一条直线，所以自然而然的会出现梯度。
   同理，tanh也是一样的现象。

23.Relu系列的激活函数相对于Sigmoid和Tanh激活函数的优点是什么？他们有什么局限性和改进？
   Relu相对于Sigmoid的优点在于Relu在于relu不会出现梯度消失的现象，但它的缺点是永远不会出现负的值，如果是负的值，那么就不会被激活。这样的话，会产生一种现象，大量的神经元会发生坏死。然后有一种Rrelu,而且Rrelu还在一定程度上能起到正则化的作用。

24.多层感知机的反向传播算法。
 
   写出多层感知机的平方误差和交叉熵损失函数。
   1/2(y-l)^2的和再除以m
   那么交叉熵其实是y*lno +(1-y)ln(1-o) 对于整体来说，就是y*lno

2.根据问题1中定义的损失函数，推导各层参数更新的梯度计算公式。
   梯度的计算公式，其实是根据损失y,反向给下一层。
   在计算的过程中，y只和当前的损失，以及实际值x有关，现在让我们来学一下推导。
   

 






1.投递搜狗
2.投递阅文
3.东方财富

1. 2019年4月23日 19：00  周二  美团点评，笔试



