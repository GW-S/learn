《自然语言处理知识整理》
1.神经网络的训练技巧：
	神经网络解决过拟合问题：
		1.数据集增强
		2.正则化/参数范数惩罚
		3.模型集成
		4.Dropout
		5.学习率，权重衰减系数，Dropout比例
		6.批量归一化：(Batch Normalization) BN
		

	1.神经网络训练时是否可以将全部参数初始化为0？
	2.为什么Dropout可以抑制过拟合？它的工作原理和实现？
	3.学习到的原始数据分布到底是什么？


GRU比LSTM又有什么不同呢？
	LSTM：LSTM 有三个门，一个输入门，一个输出门，一个遗忘门
        三个门的值都来自于Input和h层，然后是输入门判断有多少可以输入，记忆门判断有多少可以加入，加入前要tanh一下，而后是输出门，判断哪些是可以输出的，输出前也要tanh一下。
	GRU： 
GRU有两个门，一个门用来重置的，一个门是用来更新的，首先根据输入和隐藏层，也即是C本身计算出输入，再用这个输入判断，我们该保留多少，该新增多少。那么从能过滤进来的门中结合现有的，然后tanh一下，决定保留多少，输出多少。最后的值都可以说是下一层的隐藏值，下一层的输出值。
	GRU和LSTM的正向传导。
	只要把公式写出来，一层一层的传导，就可以了。
	LSTM的反向传播：

感知机的反向传播？
感知机的反向传播是很暴躁的反向传播，理论上直接一层一层的套下去，最终生成我们的目标。但是为了去除重复项，可以计算每一层的损失，那就是上一层的值来/这一层的参数的值，如此，就能求出当前的损失来，根据当前的损失，不断就行迭代。
反向传播的过程中，实际上是需要一个实际值和一个损失值的，但是实际值人不会记得，W会被记得，所以需要算出实际值，有了实际值，就能算出w该在这个地方下降多少。

	其实求得是在某一层对某个值的偏导数，那么正好将最后的损失分配到了当前这一层的这个点，正好是上一层的损失*产生损失的函数在这一点的偏导数，结果就是上一层的损失。

CNN 是个怎样的过程呢？
首先是直接embedding,embeddind后和一个长度为w的层相乘，一般情况下是点击，生成n-gram凝聚成的一个词，再之后就是进行池化，从这些凝聚之后的结果根据不同挑选最大值和最小值。

LSTM的反向传播的推导点就在于两个过程，一个是针对于当前隐藏节点的推导，一个是针对于下一层的损失函数进行的推导，但难点在当前层h的节点的损失，会影响到下一层C节点的损失，
所以就要先计算出两个损失L，一个是t位置的损失，一个是L（t+1),因为L（t+1)影响到C，但C又影响到l，那么破局的方法，就是想办法记录C在每一步的值，然后达到计算的目的。

CNN则是怎么解决的呢？
     CNN的面对的问题是，它问题是有一个卷积层，还有一个池化层，池化层怎么获得损失函数呢？那知识一个平均而已，我们将其设置为y=z，那么其导致就为1.同样池化层上一层的损失怎么求呢？我们可以对池化层根据它的池化方式进行扩展，比如左上，右上，左下为最大值，其他部分填充为0.那么卷积层怎么处理呢？卷积层其实也就是普通的传播网络，但它的连接方式是一个卷积，针对所有的应用，那么损失就可以被分薄给所有连接该点的值，表现形式就是卷积核被翻转180度，再乘回去。对于一个z,就由数个点决定，那么再根据决定的公式，再次进行优化就行了。

transformer篇
1.	
	

	LSTM的前向传播：
	GRU：

CNN是如何运作的？
   CNN同样是一层一层

在神经网络的反向传播中，我们会发现，每一次传播的目的是更新损失函数，损失函数是很好更新的，就是根据最后一点的损失不断求梯度。




batchNormal:batchNormal是一个公式，它的核心思想是，在传播的过程中，我们会发现，由于sigmoid的形变，会导致梯度消失的现象，同样，在过程中，会发生分布变形的情况。
所以，就有了batchNormal,他会计算一个batch中的所有值的平均值，再计算所有的方差，最后对每个x减去平均值和方差，最后得到x,而y为了保证方差不变，设置了两个参数，一个的塔，一个瑟塔，保证batcHormal的安全。

	

