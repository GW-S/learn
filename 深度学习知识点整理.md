1. 反向传播算法；
在反向传播算法中，实质上是一个三步走。
第一步，计算关于要更新的参数的导数。
第二步，计算每一层的损失的函数。
第三部，在每一次应用第一步，第二步计算出来的公式，这是为了提高计算效率。
我手写已经完成。
题目：给一个反向传播的问题，能找到反向传播问题的解。

2. 深度学习的超参数优化。
在深度学习的训练中，主要有四个问题
1.病态，2.鞍点  3.局部最小值 4.山谷 5.梯度悬崖
对于这些情况，人们常用局部最小值来判断，实质上当参数变动很大的时候，损失还是上不去的时候，那就不是局部最小值。如果参数变动不大，那么是鞍点，如果梯度频繁变动，那有可能是山谷。如果梯度悬崖，那就是梯度会瞬间变得极大，爬都爬不过去；

在这个过程中人们提出了很多方法来解决。
比如：
del开头的某算法:它的意思是如果某几次求导，它的值都保持是增量，那么它的学习率应该增大。

momente:它的意思是如果根据以前的速度v,还有这一次的梯度，一起决定更新，这样的话，就能防止在山谷里面滚来滚去。

AdaGrad:是一种根据过去的历史，来优化当前的梯度，如果过去某梯度更新，那就记录它，然后计算平方梯度的值，用现在的值/过去的平方梯度开根号，得到现在的更新量。

RMS算法:面对碗状的形态，adaGrad在一个方向走，越走步子越小，那么就来一个和LSTM很相似的方法，遗忘一部分过去的值，加上一部分现在的梯度的平方，然后相减。

Nesterov:这在RMS算法，在RMS的基础上，保存了一个v,相当于momente加上nesterov

Adam算法：该算法同时考虑了一阶和二阶，一阶二阶都加入了遗忘机制，还进行了一定程度的修正，最后输出了s和r的减。

学习和优化有什么不同？ 学习的目的是优化性能度量，而优化的过程中，我们的评价标准往往不能直接使用，


大概有三种思想
1.是AdaGrad思想，这种思想的特点是，针对动量，我们还要关注他们的平方，比如v + g * g,在这种情况下，我们能够计算平方，然后用ag/b+根号下r * g,来获得相应的思想。
2.moment,思想是利用以前的速度v,计算梯度v-a￥，从而计算梯度。
3.再之后是，Nesterov动量，这个东西是利用提前更新动量的方法来做的。
4.   还有RMSpro,这个东西是为了防止adagrad以往的速度太快，所以字啊计算根号下部分的时候，用p(r)-(1-p)g*g来做出一个更新。
最后是adam,他是i结合一阶动量和二阶动量的一个东西，s= p_1s + (1-p1)g 和 r = p2r + (1-p2)g * g.然后再修正偏差，比如s/1-pt  比如r/1-pt ,那么求出一个新的骗到，& = -￥ s/(^r + @)


KNN 是干什么的？
这其实是一个分类任务，在K个里面寻找最进的几个。这就是KNN
那么DBScan是基于密度的聚类，其实质是在一个核心集内找到可达集的过程。







# 权重衰减系数
https://zhuanlan.zhihu.com/p/38709373
